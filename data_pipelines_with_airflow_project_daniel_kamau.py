# -*- coding: utf-8 -*-
"""Data Pipelines with Airflow Project_Daniel Kamau.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HgT8pgrljo_uRFwf_K2LnPN1DtJm-QpQ

### **Data Pipelines with Airflow Project_Daniel Kamau**

**Background Information**

Our telecommunications company, MTN Rwanda, has a vast customer base, and we generate a large amount of data daily. We must efficiently process and store this data to make informed business decisions. Therefore, we plan to develop a data pipeline to extract, transform, and load data from three CSV files and store it in a Postgres database. We require a skilled data engineer who can use the Airflow tool to develop the pipeline to achieve this.

**Problem Statement**

The main challenge is that the data generated is in a raw format, and we need to process it efficiently to make it usable for analysis. This requires us to develop a data pipeline that can extract, transform and load the data from multiple CSV files into a single database, which can be used for further analysis.

**Guidelines**

The data pipeline should be developed using Airflow, an open-source tool for creating and managing data pipelines. The following steps should be followed to develop the data pipeline:

● The data engineer should start by creating a DAG (Directed Acyclic Graph) that defines the workflow of the data pipeline.

● The DAG should include tasks that extract data from the three CSV files.

● After extraction, the data should be transformed using Python libraries to match the required format.

● Finally, the transformed data should be loaded into a Postgres database.

● The data pipeline should be scheduled to run at a specific time daily using the Airflow scheduler.

● We can use the shared file (mtnrwanda-dag.py) as a starting point.


**Sample CSV Files**

The following are sample CSV files that will be used in the data pipeline:

● customer_data.csv

● order_data.csv

● payment_data.csv

All files for this project can be downloaded from here (link).


**Deliverables**

We will be expected to deliver a GitHub repository with the following:

● Airflow DAG file for the data pipeline.

● Documentation of the pipeline.

○ Highlight at least 3 best practices used during the implementation.

○ Recommendations for deployment and running the pipeline in a cloud-based
provider.
"""

!pip install apache-airflow-providers-postgres

#Import necessary libraries
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import psycopg2
import logging

# Function to connect to Postgres database
def get_pg_connection():
    conn = psycopg2.connect(database="my_postgres_db", user="postgres", password="E*3b8km$dpmRLLuf1Rs$", host="157.245.102.81", port="5432")
    return conn

# Function to extract data from CSVs
def extract():
    customer_df = pd.read_csv('customer_data.csv')
    order_df = pd.read_csv('order_data.csv')
    payment_df = pd.read_csv('payment_data.csv')
    return customer_df, order_df, payment_df

# Function to transform data
def transform(customer_df, order_df, payment_df):

    # Merge the customer and order dataframes on the customer_id column, then payment on the order_id and customer_id columns
    order_df1 = pd.merge(customer_df, order_df, on='customer_id')

    payment_df1 = pd.merge(order_df1, payment_df, on=['order_id', 'customer_id'])

    #Drop unnecessary columns
    payment_df1.drop(columns=['customer_id', 'order_id'], inplace=True)

    # Group the data by customer and aggregate the amount paid using sum
    transformed_data = payment_df1.groupby(['first_name', 'last_name', 'email', 'country', 'gender', 'date_of_birth'])['amount'].sum().reset_index()

    # create a new column and assign it the total value of orders made by each customer
    transformed_data['agg_orders'] = transformed_data.groupby('first_name')['amount'].transform('sum')

    # Calculate the customer lifetime value
    transformed_data['clv'] = transformed_data['amount'] * 1 * 1

    return transformed_data

# Function to load data into Postgres database
def load(transformed_data):
    conn = get_pg_connection()
    cur = conn.cursor()
    # Create table if it doesn't exist
    cur.execute("CREATE TABLE IF NOT EXISTS mtn_customer (customer_id INTEGER PRIMARY KEY, first_name,last_name,email,country,gender,date_of_birth_str,product,price,order_date_str,payment_id,amount,payment_date_str,agg_orders,clv)")
    # Insert data into table
    for data in transformed_data:
        cur.execute("INSERT INTO mtn_customer (customer_id, first_name,last_name,email,country,gender,date_of_birth_str,product,price,order_date_str,payment_id,amount,payment_date_str,agg_orders,clv) VALUES (%s, %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)",
                    ((data['customer_id'], data['first_name'],data['last_name'],data['email'],data['country'],data['gender'],data['date_of_birth_str'],data['product'],data['price'],data['order_date_str'],data['payment_id'],data['amount'],data['payment_date_str'],data['agg_orders'],data['clv'])))

    #Commit and close the cur and connection
    conn.commit()
    cur.close()
    conn.close()

# Define the default arguments for the DAG
default_args = {
    'owner': 'MTN Rwanda Telecom Company',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# Define the DAG
dag = DAG(
          'data_pipeline',
          default_args=default_args,
          description='Extract, transform, and load data from three CSVs into a Postgres database',
          schedule_interval='@daily',
          )

# Define tasks
extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract,
    dag=dag,
)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform,
    op_kwargs={
        'customer_df': extract_task.output[0],
        'order_df': extract_task.output[1],
        'dpayment_df': extract_task.output[2],
    },
    dag=dag,
)

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load,
    op_kwargs={
        'transformed_data': transform_task.output
    },
    dag=dag,
)

# Set task dependencies
transform_task.set_upstream(extract_task)
load_task.set_upstream(transform_task)

"""**Hosting Recommendation**

This can be hosted on managed cloud services through Amazon Web Services (AWS) or google cloud as they offer AWS(EC2 and ECS) and Google Kubernetes Engine(GKE) respectively that are compatible and can host Airflow.
this would simplify deployment and management by providing features such as managed DBs, automatic scaling, monitoring, and high availability
"""